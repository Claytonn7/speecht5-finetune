{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-22T00:23:10.148045Z",
     "start_time": "2024-10-22T00:23:05.766995Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import SpeechT5ForTextToSpeech, SpeechT5Tokenizer\n",
    "\n",
    "# Load pre-trained SpeechT5 model and tokenizer\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "tokenizer = SpeechT5Tokenizer.from_pretrained(\"microsoft/speecht5_tts\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1764, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1206, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1178, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1149, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\speecht5\\modeling_speecht5.py\", line 22, in <module>\n",
      "    import torch.utils.checkpoint\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\checkpoint.py\", line 24, in <module>\n",
      "    from torch._functorch._aot_autograd.functional_utils import is_fun\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\functional_utils.py\", line 19, in <module>\n",
      "    from torch.fx.experimental.symbolic_shapes import definitely_true, sym_eq\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py\", line 64, in <module>\n",
      "    from torch.utils._sympy.functions import (\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_sympy\\functions.py\", line 6, in <module>\n",
      "    import sympy\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\__init__.py\", line 74, in <module>\n",
      "    from .polys import (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\polys\\__init__.py\", line 78, in <module>\n",
      "    from .polyfuncs import (symmetrize, horner, interpolate,\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\polys\\polyfuncs.py\", line 10, in <module>\n",
      "    from sympy.polys.specialpolys import (\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\polys\\specialpolys.py\", line 298, in <module>\n",
      "    from sympy.polys.rings import ring\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\polys\\rings.py\", line 30, in <module>\n",
      "    from sympy.printing.defaults import DefaultPrinting\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\printing\\__init__.py\", line 5, in <module>\n",
      "    from .latex import latex, print_latex, multiline_latex\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\printing\\latex.py\", line 18, in <module>\n",
      "    from sympy.tensor.array import NDimArray\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\tensor\\__init__.py\", line 4, in <module>\n",
      "    from .indexed import IndexedBase, Idx, Indexed\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\tensor\\indexed.py\", line 114, in <module>\n",
      "    from sympy.functions.special.tensor_functions import KroneckerDelta\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\functions\\__init__.py\", line 35, in <module>\n",
      "    from sympy.functions.special.singularity_functions import SingularityFunction\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\functions\\special\\singularity_functions.py\", line 7, in <module>\n",
      "    from sympy.functions.special.delta_functions import Heaviside\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\functions\\special\\delta_functions.py\", line 8, in <module>\n",
      "    from sympy.polys.polyroots import roots\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sympy\\polys\\polyroots.py\", line 24, in <module>\n",
      "    from sympy.polys.polyquinticconst import PolyQuintic\n",
      "  File \"<frozen importlib._bootstrap>\", line 1178, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1149, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1032, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1131, in get_data\n",
      "MemoryError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_46264\\2490486415.py\", line 2, in <module>\n",
      "    from transformers import SpeechT5ForTextToSpeech, SpeechT5Tokenizer\n",
      "  File \"<frozen importlib._bootstrap>\", line 1231, in _handle_fromlist\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1755, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1754, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1766, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import transformers.models.speecht5.modeling_speecht5 because of the following error (look up to see its traceback):\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2168, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1457, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1348, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1195, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1085, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1182, in get_records\n",
      "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\utils.py\", line 83, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 587, in mapper\n",
      "    return cls(f, options)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stack_data\\core.py\", line 551, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\executing\\executing.py\", line 264, in executing\n",
      "    source = cls.for_frame(frame)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\executing\\executing.py\", line 183, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\executing\\executing.py\", line 212, in for_filename\n",
      "    return cls._for_filename_and_lines(filename, tuple(lines))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\executing\\executing.py\", line 223, in _for_filename_and_lines\n",
      "    result = source_cache[(filename, lines)] = cls(filename, lines)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\executing\\executing.py\", line 163, in __init__\n",
      "    self.tree = ast.parse(self.text, filename=filename)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py\", line 50, in parse\n",
      "    return compile(source, filename, mode, flags,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "MemoryError\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Define the pruning method and parameters\n",
    "pruning_method = prune.L1Unstructured\n",
    "pruning_amount = 0.2  # Prune 20% of connections\n",
    "\n",
    "# Apply pruning to desired layers\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=pruning_amount)\n",
    "        # Optionally remove the pruning re-parametrization to make it permanent\n",
    "        prune.remove(module, 'weight')\n"
   ],
   "id": "f6db1c3561961453",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"D:/lalwani/tts-finetuning/pruned-models/pruned_speecht5\")\n",
    "tokenizer.save_pretrained(\"D:/lalwani/tts-finetuning/pruned-models/pruned_speecht5\")\n"
   ],
   "id": "989d3bbcc609e687",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from datasets import load_dataset\n",
    "# \n",
    "# dataset = load_dataset('csv', data_files=r'D:\\lalwani\\tts-finetuning\\pythonPro\\audio_data.csv', split='train')"
   ],
   "id": "bb4358722ebfa5a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset",
   "id": "8f4502fd8e1ec1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T00:23:36.601989Z",
     "start_time": "2024-10-22T00:23:35.703521Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech",
   "id": "b71b39d168d0fcf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-22T00:23:38.283744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the fine-tuned SpeechT5 model\n",
    "model_path = \"D:/lalwani/tts-finetuning/pruned-models/pruned_speecht5\"\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(model_path)\n",
    "\n",
    "# Load the corresponding tokenizer\n",
    "tokenizer = SpeechT5Tokenizer.from_pretrained(model_path)\n"
   ],
   "id": "795575067e03af70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "checkpoint = \"microsoft/speecht5_tts\"\n",
    "processor = SpeechT5Processor.from_pretrained(checkpoint)"
   ],
   "id": "fa19442fdcc4b349",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T00:23:10.154329Z",
     "start_time": "2024-10-22T00:23:10.153819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset('csv', data_files=r'D:\\lalwani\\tts-finetuning\\pythonPro\\final_dataset.csv')\n",
    "print(dataset['train']['file_path2'])"
   ],
   "id": "974da633ba87e57c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T00:23:10.154856Z",
     "start_time": "2024-10-22T00:23:10.154329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = dataset.remove_columns('original_file_path')\n",
    "print(dataset)"
   ],
   "id": "81c540e38fb9aac8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T00:23:10.155381Z",
     "start_time": "2024-10-22T00:23:10.155381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import soundfile as sf\n",
    "import os"
   ],
   "id": "7626ed1a8b980ebe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T00:23:10.155381Z",
     "start_time": "2024-10-22T00:23:10.155381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_audio_data(folder_path):\n",
    "    data_list = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.wav'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            file_path = os.path.normpath(file_path)\n",
    "            print(file_path)\n",
    "            audio_array, samplerate = sf.read(file_path)\n",
    "            data = {\n",
    "                'audio_id': os.path.splitext(os.path.basename(file_path))[0],\n",
    "                'audio': {\n",
    "                    'path': file_path,\n",
    "                    'array': audio_array,\n",
    "                    'sampling_rate': 16000\n",
    "                }\n",
    "            }\n",
    "            data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "# Usage\n",
    "base_path_final = r\"D:\\lalwani\\tts-finetuning\\data-finetuning\\extracted_data\\mono_combined\\resampled\" \n",
    "base_path_final = os.path.normpath(base_path_final) \n",
    "file_paths = dataset['train']['file_path2']\n",
    "\n",
    "audio_data = get_audio_data(base_path_final)"
   ],
   "id": "80cd5537050098ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_gender_to_audio_data(audio_data):\n",
    "    for data in audio_data:\n",
    "        filename = data['audio_id'].lower()\n",
    "        if 'female' in filename:\n",
    "            data['gender'] = 'female'\n",
    "        elif 'male' in filename:\n",
    "            data['gender'] = 'male'\n",
    "        else:\n",
    "            data['gender'] = 'unknown'\n",
    "    return audio_data\n",
    "\n",
    "\n",
    "audio_data_with_gender = add_gender_to_audio_data(audio_data)"
   ],
   "id": "cbf1d4ea9ff8cb1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset['train']['englishText']",
   "id": "8fcd684c44c10591",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_transcription_to_audio_data(audio_data_with_gender, english_text):\n",
    "    if len(audio_data_with_gender) != len(english_text):\n",
    "        print(\"Warning: The lengths of audio_data_with_gender and english_text do not match.\")\n",
    "        print(f\"audio_data_with_gender length: {len(audio_data_with_gender)}\")\n",
    "        print(f\"english_text length: {len(english_text)}\")\n",
    "    \n",
    "    for i, audio_item in enumerate(audio_data_with_gender):\n",
    "        if i < len(english_text):\n",
    "            audio_item['transcription'] = english_text[i]\n",
    "        else:\n",
    "            print(f\"Warning: No matching transcription for audio item at index {i}\")\n",
    "    \n",
    "    return audio_data_with_gender\n",
    "\n",
    "# Use the function to add transcriptions to your audio data\n",
    "audio_data_with_transcription = add_transcription_to_audio_data(audio_data_with_gender, dataset['train']['englishText'])\n",
    "\n",
    "# Print a sample to verify\n",
    "print(audio_data_with_transcription[0])"
   ],
   "id": "7a2b61647c4b423c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "batch_size = 100  # Adjust this based on your available memory\n",
    "dataset = None\n",
    "\n",
    "for i in range(0, len(audio_data_with_transcription), batch_size):\n",
    "       batch = audio_data_with_transcription[i:i+batch_size]\n",
    "       batch_dataset = Dataset.from_list(batch)\n",
    "       \n",
    "       if dataset is None:\n",
    "           dataset = batch_dataset\n",
    "       else:\n",
    "           dataset = concatenate_datasets([dataset, batch_dataset])"
   ],
   "id": "ef74055525981a71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset",
   "id": "eb4015084cc6a728",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
    ")\n",
    "\n",
    "\n",
    "def create_speaker_embedding(waveform):\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
    "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
    "    return speaker_embeddings"
   ],
   "id": "264bdade69d4ffe1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    example = processor(\n",
    "        text=example[\"transcription\"],\n",
    "        audio_target=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "\n",
    "    # strip off the batch dimension\n",
    "    example[\"labels\"] = example[\"labels\"][0]\n",
    "\n",
    "    # use SpeechBrain to obtain x-vector\n",
    "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n",
    "\n",
    "    return example"
   ],
   "id": "ffb7b338fe112c29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "processed_example = prepare_dataset(dataset[0])\n",
    "list(processed_example.keys())"
   ],
   "id": "d756243618691dbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed_example[\"speaker_embeddings\"].shape",
   "id": "8126177027322a0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(processed_example[\"labels\"].T)\n",
    "plt.show()"
   ],
   "id": "449e6b6b4cbfc361",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)",
   "id": "63dd7623b6cad91a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def is_not_too_long(input_ids):\n",
    "    input_length = len(input_ids)\n",
    "    return input_length < 200\n",
    "\n",
    "\n",
    "dataset3 = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n",
    "len(dataset3)"
   ],
   "id": "d18a813f194c19ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset3 = dataset3.train_test_split(test_size=0.1)",
   "id": "b7924e198d0d707c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TTSDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
    "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
    "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
    "\n",
    "        # collate the inputs and targets into a batch\n",
    "        batch = processor.pad(\n",
    "            input_ids=input_ids, labels=label_features, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n",
    "            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n",
    "        )\n",
    "\n",
    "        # not used during fine-tuning\n",
    "        del batch[\"decoder_attention_mask\"]\n",
    "\n",
    "        # round down target lengths to multiple of reduction factor\n",
    "        if model.config.reduction_factor > 1:\n",
    "            target_lengths = torch.tensor(\n",
    "                [len(feature[\"input_values\"]) for feature in label_features]\n",
    "            )\n",
    "            target_lengths = target_lengths.new(\n",
    "                [\n",
    "                    length - length % model.config.reduction_factor\n",
    "                    for length in target_lengths\n",
    "                ]\n",
    "            )\n",
    "            max_length = max(target_lengths)\n",
    "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
    "\n",
    "        # also add in the speaker embeddings\n",
    "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
    "\n",
    "        return batch"
   ],
   "id": "6b48a38b15c03cf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_collator = TTSDataCollatorWithPadding(processor=processor)",
   "id": "40918c37e5870f20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ],
   "id": "b2e9d1fb77fdb21a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d9fbbed0db0c1b36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "\n",
    "# disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# set language and task for generation and re-enable cache\n",
    "model.generate = partial(model.generate, use_cache=True)"
   ],
   "id": "d0a114e4c096112b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"speecht5_finetuned_hindi_mono\",  \n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    label_names=[\"labels\"],\n",
    "    push_to_hub=True,\n",
    ")"
   ],
   "id": "6527ec3f7c5ff00e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install tf-keras",
   "id": "d5685c72ad5c9b76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset3[\"train\"],\n",
    "    eval_dataset=dataset3[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor,\n",
    ")"
   ],
   "id": "3999674c6457cd32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "cefccc8712370e98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.push_to_hub()",
   "id": "51e40b253df6c0d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"hola\")",
   "id": "c739f7ac54573a4e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
